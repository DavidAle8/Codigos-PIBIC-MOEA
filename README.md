# Codigos-PIBIC-MOEA

Estes c√≥digos fazem parte dos estudos de problemas de otimiza√ß√£o multiobjetivo (MOP, do ingl√™s Multi-Objective Optimization Problems) na qual algoritmos multiobjetivo s√£o utilzados para solucion√°-los.

# Introdu√ß√£o

A otimiza√ß√£o √© uma √°rea amplamente pesquisada na ci√™ncia da computa√ß√£o,
engenharia e matem√°tica que visa buscar a melhor solu√ß√£o para um problema espec√≠fico
baseado em crit√©rios bem definidos. Dessa forma, podemos dizer que a √°rea busca
maximizar ou minimizar uma fun√ß√£o objetivo respeitando as restri√ß√µes do problema. Mas,
e se tiv√©ssemos mais de um objetivo a ser avaliado simultaneamente? Os problemas de
otimiza√ß√£o multiobjetivo (MOP, do ingl√™s Multi-Objective Optimization Problems) s√£o
uma classe de problemas com dois ou mais fun√ß√µes objetivos a serem otimizados de forma
simult√¢nea, no qual os mesmos podem ter conflitos entre si (COELLO et al., 2007 apud
OLIVEIRA, 2022). Por conta desses conflitos entre os objetivos, n√£o existe apenas uma
melhor solu√ß√£o a ser considerada que possa otimizar totalmente os mesmos, mas sim um
conjunto delas, que chamamos de solu√ß√µes n√£o-dominadas. Dessa maneira, para obtermos
esse conjunto de solu√ß√µes, √© necess√°rio aplicarmos o que chamamos de Teoria da
Otimalidade de Pareto (COELLO et al., 2007 apud OLIVEIRA, 2022).
Os MOPs s√£o problemas complexos, dessa maneira, Algoritmos Evolutivos
Multiobjetivo (MOEA, do ingl√™s Multi-Objective Evolutionary Algorithm) s√£o algoritmos
inspirados na teoria evolucion√°ria da biol√≥gica para busca dos objetivos com maior
aptid√£o. Estes algoritmos t√™m se sobressa√≠do e bem aplicados para estes problemas.
Segundo (D√çAZ-MANR√çQUEZ et al., 2016) os MOEAs s√£o t√©cnicas baseadas em
popula√ß√µes que realizam buscas multidimensionais, encontrando mais de uma solu√ß√£o em
uma √∫nica execu√ß√£o. Estas t√©cnicas possuem um potencial muito grande quando estamos
falando em buscar solu√ß√µes pr√≥ximas dos √≥timos globais (solu√ß√µes pr√≥ximas das
melhores). Muitos algoritmos que utilizam do conceito da evolu√ß√£o da biologia s√£o
caracter√≠sticas dos MOEAs. As solu√ß√µes s√£o indiv√≠duos de uma popula√ß√£o, e os mesmos
cruzam entre si para gerar uma popula√ß√£o nova. A partir disso os indiv√≠duos sofrem
muta√ß√µes, e isso auxilia na variabilidade dos mesmos para gerar diferentes solu√ß√µes nas
pr√≥ximas gera√ß√µes.

# Desenvolvimento

A pesquisa separou o desenvolvimento em duas etapas: A primeira foi
estudado e compreendido a implementa√ß√£o de algoritmos mono-objetivo com o
intuito de entender a otimiza√ß√£o. E na segunda etapa compreendeu-se a avalia√ß√£o de
algoritmos surrogates multi-objetivo. A pesquisa n√£o alcan√ßou a implementa√ß√£o de
novos algoritmos, apenas concentrou-se na execu√ß√£o do mesmo no framework
disponibilizado.
Antes de irmos para os algoritmos propriamente ditos, falaremos brevemente
das fun√ß√µes que o comp√µem para uma melhor demonstra√ß√£o e melhor entendimento.
Nesta pesquisa usamos como fun√ß√£o objetivo fun√ß√µes de benchmark da literatura,
sendo elas Sphere e Rastrigin

<img width="344" height="176" alt="image" src="https://github.com/user-attachments/assets/0994ab36-4c85-4af1-8f55-67aeeb9d7eb7" />

## Hill-Climbing

O algoritmo √© simples e funciona da seguinte forma: o mesmo recebe uma
lista de 50 elementos como par√¢metro (um vetor de n√∫meros gerados aleatoriamente
entre -100 e 100), uma quantidade de itera√ß√µes (avalia√ß√µes), valor da probabilidade
de um n√∫mero ser alterado (p) e um valor de ru√≠do a ser aplicado (r). Primeiro
fazemos S recebe a nossa lista de elementos atual. Dessa maneira, para cada
avalia√ß√£o, fazemos uma c√≥pia de S em R aplicando um pequeno ru√≠do em seus
elementos. Se R ap√≥s o ru√≠do aplicado for melhor que S, ent√£o S recebe esta melhor
solu√ß√£o. Como estamos trabalhando com minimiza√ß√£o, ent√£o a melhor solu√ß√£o ser√°
a menor delas.

## Simulated Annealing

O algoritmo funciona da seguinte forma: Primeiro recebemos uma lista no mesmo
formato que o algoritmo Hill-Climbing, um vetor de 50 elementos variando de -100 a 100.
Copiamos essa lista de elementos para S, Best recebe S para termos uma solu√ß√£o inicial a
ser comparada com as demais e P √© o valor de probabilidade que come√ßa alto (100%).
Enquanto a temperatura n√£o esfria (n√£o fica um valor muito pequeno) ou a quantidade de
itera√ß√µes for maior que 0 (nossas avalia√ß√µes), rodamos a l√≥gica do algoritmo. Como de
rotina, R recebe uma c√≥pia modificada de S inicialmente e a vari√°vel expoente recebe parte
da f√≥rmula que apresentamos da probabilidade P. O algoritmo verifica a condi√ß√£o se
Quality(S) < Quality(R), pois se isso √© satisfeito, significa que S √© melhor que R, ou seja,
como R √© pior, ent√£o fazemos o c√°lculo de probabilidade de P recebendo ùëÉ = ùëí (a ùëíùë•ùëùùëúùëíùëõùë°ùëí
ferramenta math.exp do python garante o ùëí ). Ap√≥s isso geramos um valor ùëíùë•ùëùùëúùëíùëõùë°ùëí
num_random aleat√≥rio para a verifica√ß√£o de probabilidade e em seguida verificamos a
condi√ß√£o: ‚Äúse o Quality(R) < Quality(S) ou num_random < P, fa√ßa‚Äù. Se a condi√ß√£o do if
anterior, Quality(S) < Quality(R) tivesse sido satisfeito antes, ent√£o quer dizer que S √©
melhor que R, assim, Quality(R) < Quality(S) na condi√ß√£o atual n√£o seria verdadeiro,
sobrando apenas para condi√ß√£o de num random < P para que o comando submisso seja
executado, mas com condi√ß√£o de probabilidade de aceitar solu√ß√µes ruins baseado no
c√°lculo da temperatura. Dessa maneira, S recebe o valor ruim de R, guardando solu√ß√µes
piores. O t = max(0.05, t) significa que o menor valor de t ser√° no m√≠nimo 0,05 e
diminu√≠mos ele subtraindo-o de t/itera√ß√µes, ou seja, ‚Äúesfriamos‚Äù ele lentamente para
assegurar que a condi√ß√£o de aceitar resultados ruins de R aconte√ßa at√© que a validade disso
acaba, logo, sobrando apenas para aceitar solu√ß√µes melhores e colocando em Best na
condi√ß√£o: ‚Äúse o Quality(S) < Quality(Best)‚Äù, assim, Best recebe S e o retornamos.

##  Evolution Strategy

O algoritmo trabalha da seguinte forma: recebemos como par√¢metro uma popula√ß√£o
de indiv√≠duos, pais que representar√£o a quantidade de indiv√≠duos aptos para reprodu√ß√£o e
filhos, quantidade de descendentes desses pais. O Best inicialmente recebe o pior
indiv√≠duo poss√≠vel inicialmente para termos um indiv√≠duo inicial substitu√≠vel e Q, nossa
lista dos melhores indiv√≠duos, come√ßa vazia inicialmente. Ap√≥s isso geramos uma
popula√ß√£o aleat√≥ria, uma lista de 50 indiv√≠duos com um genoma de tamb√©m tamanho 50
com elementos variando de -100 a 100. Em seguida percorremos cada indiv√≠duo dessa lista
avaliando seu fitness com o assess_fitness e verificando se o fitness do indiv√≠duo que
estamos avaliando agora √© melhor que o de Best, se for, Best recebe este indiv√≠duo fazendo
uma c√≥pia dele. Com todas essas avalia√ß√µes feitas, fazemos Q receber esses indiv√≠duos
avaliados, ordenando eles do menor para o maior, ou seja, do melhor pro pior, e pegamos
os pais melhores e colocamos em uma outra lista dos mais aptos, nesse caso
intuitivamente na lista mais_aptos. A ideia de limpar Q e populacao ser√° para uma
pr√≥xima rodada com uma nova gera√ß√£o de indiv√≠duos. E por fim, percorremos cada
indiv√≠duo apto da nossa lista dos mais aptos para gerar indiv√≠duos novos pela quantidade
da express√£o filhos//pais (divis√£o inteira de filhos e pais), ou seja, praticamente, para cada
indiv√≠duo apto da lista, (os pais melhores, digamos assim), √© gerado um novo indiv√≠duo
com uma muta√ß√£o aplicada (um ajuste em seus elementos).

# Resultados

## Resultados Hill-Climbing

<img width="1408" height="454" alt="image" src="https://github.com/user-attachments/assets/a380c098-2d18-452e-b6a0-d098b4bfba35" />

Na tabela podemos ver que, tanto no Sphere quanto no Rastrigin
obtivemos as melhores solu√ß√µes usando a configura√ß√£o com itera√ß√£o =
10000, p = 0.5 e r = 1. O segundo melhor valor de ambas as fun√ß√µes
objetivo se concentraram nas mesmas configura√ß√µes de p e r, apenas
havendo mudan√ßa na quantidade de itera√ß√µes que foi 50000, por√©m se
percebermos, n√£o mudou tanto em rela√ß√£o ao primeiro resultado, o que nos
diz que este seria um teto interessante para os melhores resultados
Hill-Climbing com estas configura√ß√µes. Ap√≥s isso n√£o obtivemos resultados 
interessantes. Sendo assim, podemos concluir que p = 0.5 e r = 1 foram os
MVPs das melhores solu√ß√µes, independente do valor de itera√ß√£o.

## Resultados Simulated Annealing

<img width="1151" height="364" alt="image" src="https://github.com/user-attachments/assets/a60239b2-42e1-49f2-a98d-76422de521e7" />

Para estes resultados, devemos lembrar que o Recozimento Simulado
possui um par√¢metro a mais dos que foram definidos, no caso o da
temperatura t que foi configurada da seguinte forma: t = 5000, 3000 e 1000.
Partindo agora para os resultados, podemos ver tamb√©m que os melhores
deles permearam-se nas configura√ß√µes de itera√ß√£o = 50000, p = 0.5, r = 1 e
agora com t = 3000. Como segundo melhor resultado obtivemos com os
mesmos valores de p, r e t mas com itera√ß√£o = 10000. Por√©m, perceba
tamb√©m que, diferente do Hill-Climbing, o Recozimento Simulado melhora
seus resultados √† medida que o n√∫mero de interna√ß√µes cresce. Dessa
maneira, podemos concluir que o Simulated Annealing precisa de mais
explora√ß√µes para alcan√ßar melhores resultados.

## Resultados Evolution Strategy

E por fim, os resultados do algoritmo Evolution Strategy, que
tamb√©m possui par√¢metros espec√≠ficos pais e filhos, que foram configurados
da seguinte forma: pais = 5, 15 e 25, e filhos = 10, 100 e 250.

<img width="821" height="296" alt="image" src="https://github.com/user-attachments/assets/3cd39e76-a913-4c78-a8d9-8d54b8a20a1f" />

Diante dos resultados, j√° podemos visualizar que tamb√©m tivemos os
melhores resultados nas configura√ß√µes gerais de itera√ß√£o = 50000, p = 0.5 e
r = 1, e adicionalmente, as configura√ß√µes do algoritmo, pais = 15 e filhos =
100. Podemos averiguar tamb√©m que, o Evolution Strategy, por ser um
algoritmo melhor em buscar as solu√ß√µes, n√£o obteve o melhor resultado de
todos os algoritmos na fun√ß√£o Sphere, mas se saiu melhor com a fun√ß√£o
Rastrigin. Seu segundo melhor resultado surgiu nas mesmas configura√ß√µes,
com a √∫nica diferen√ßa no valor de itera√ß√µes, que neste caso ficou em 10000,
e chegou pr√≥ximo do melhor resultado. Logo, podemos concluir ent√£o que o
algoritmo lida melhor com fun√ß√µes mais complexas.

